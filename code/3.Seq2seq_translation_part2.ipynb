{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"colab":{"name":"3.Seq2seq_translation_part2.ipynb","provenance":[{"file_id":"https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/seq2seq_translation_tutorial.ipynb","timestamp":1580187907306}],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"wAz9hAntU0vv","colab_type":"text"},"source":["## **3.Sequnce-to-sequence network with attention**\n","Install essential packages and download the train dataset (eng-fra.txt)\n"]},{"cell_type":"code","metadata":{"id":"YLvoCMdj8qle","colab_type":"code","colab":{}},"source":["# install essential package and download eng-fra.txt data\n","!pip install wget\n","import wget\n","import zipfile\n","import ssl\n","ssl._create_default_https_context = ssl._create_unverified_context\n","\n","url=\"https://download.pytorch.org/tutorial/data.zip\"\n","wget.download(url)\n","data_zip = zipfile.ZipFile(\"./data.zip\")\n","data_zip.extractall()\n","data_zip.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-nSG03_Tid0k","colab_type":"code","colab":{}},"source":["from __future__ import unicode_literals, print_function, division\n","from io import open\n","import unicodedata\n","import string\n","import re\n","import random\n","\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yTqZWE6fVuPQ","colab_type":"text"},"source":["### **Preprocess the dataset**"]},{"cell_type":"code","metadata":{"id":"Pd3bxHozid0r","colab_type":"code","colab":{}},"source":["SOS_token = 0\n","EOS_token = 1\n","\n","\n","class Lang:\n","    def __init__(self, name):\n","        self.name = name\n","        self.word2index = {}\n","        self.word2count = {}\n","        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n","        self.n_words = 2  # Count SOS and EOS\n","\n","    def addSentence(self, sentence):\n","        for word in sentence.split(' '):\n","            self.addWord(word)\n","\n","    def addWord(self, word):\n","        if word not in self.word2index:\n","            self.word2index[word] = self.n_words\n","            self.word2count[word] = 1\n","            self.index2word[self.n_words] = word\n","            self.n_words += 1\n","        else:\n","            self.word2count[word] += 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lZ9m544xid0x","colab_type":"code","colab":{}},"source":["def unicodeToAscii(s):\n","    return ''.join(\n","        c for c in unicodedata.normalize('NFD', s)\n","        if unicodedata.category(c) != 'Mn'\n","    )\n","\n","\n","def normalizeString(s):\n","    s = unicodeToAscii(s.lower().strip())\n","    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n","    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n","    return s"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TQUDK0Khid03","colab_type":"code","colab":{}},"source":["def readLangs(lang1, lang2, reverse=False):\n","    print(\"Reading lines...\")\n","\n","    # Read the file and split into lines\n","    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n","        read().strip().split('\\n')\n","\n","    # Split every line into pairs and normalize\n","    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n","\n","    # Reverse pairs, make Lang instances\n","    if reverse:\n","        pairs = [list(reversed(p)) for p in pairs]\n","        input_lang = Lang(lang2)\n","        output_lang = Lang(lang1)\n","    else:\n","        input_lang = Lang(lang1)\n","        output_lang = Lang(lang2)\n","\n","    return input_lang, output_lang, pairs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nl1VBQCwid09","colab_type":"code","colab":{}},"source":["MAX_LENGTH = 10\n","\n","eng_prefixes = (\n","    \"i am \", \"i m \",\n","    \"he is\", \"he s \",\n","    \"she is\", \"she s\",\n","    \"you are\", \"you re \",\n","    \"we are\", \"we re \",\n","    \"they are\", \"they re \"\n",")\n","\n","\n","def filterPair(p):\n","    return len(p[0].split(' ')) < MAX_LENGTH and \\\n","        len(p[1].split(' ')) < MAX_LENGTH and \\\n","        p[1].startswith(eng_prefixes)\n","\n","\n","def filterPairs(pairs):\n","    return [pair for pair in pairs if filterPair(pair)]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xWbWA0Yfid1A","colab_type":"code","colab":{}},"source":["def prepareData(lang1, lang2, reverse=False):\n","    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n","    print(\"Read %s sentence pairs\" % len(pairs))\n","    pairs = filterPairs(pairs)\n","    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n","    print(\"Counting words...\")\n","    for pair in pairs:\n","        input_lang.addSentence(pair[0])\n","        output_lang.addSentence(pair[1])\n","    print(\"Counted words:\")\n","    print(input_lang.name, input_lang.n_words)\n","    print(output_lang.name, output_lang.n_words)\n","    return input_lang, output_lang, pairs\n","\n","\n","input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n","print(random.choice(pairs))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XlabLxX7W6Kx","colab_type":"text"},"source":["### **Encoder network**\n","output some values (a hidden state and output vector) for every word from the input sentence"]},{"cell_type":"code","metadata":{"id":"amfnBXEFid1G","colab_type":"code","colab":{}},"source":["class EncoderRNN(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(EncoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","\n","        self.embedding = nn.Embedding(input_size, hidden_size)\n","        self.gru = nn.GRU(hidden_size, hidden_size)\n","\n","    def forward(self, input, hidden):\n","        embedded = self.embedding(input).view(1, 1, -1)\n","        output = embedded\n","        output, hidden = self.gru(output, hidden)\n","        return output, hidden\n","\n","    def initHidden(self):\n","        return torch.zeros(1, 1, self.hidden_size, device=device)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pg2oNMyRZv_n","colab_type":"text"},"source":["### **Decoder network with attention (practice #1)**\n","Define decoder network structure with attention and forward pass"]},{"cell_type":"code","metadata":{"id":"90SKFtM6id1P","colab_type":"code","colab":{}},"source":["class AttnDecoderRNN(nn.Module):\n","    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n","        super(AttnDecoderRNN, self).__init__()\n","        \n","        \"\"\"\n","\n","        Code Implementation Here\n","         - Decoder network structure\n","\n","        \"\"\"\n","\n","    def forward(self, input, hidden, encoder_outputs):\n","        \"\"\"\n","        \n","        Code Implementation Here\n","         - Forward pass\n","\n","        \"\"\"\n","        return output, hidden, attn_weights\n","\n","    def initHidden(self):\n","        return torch.zeros(1, 1, self.hidden_size, device=device)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UO02NM6cid1U","colab_type":"code","colab":{}},"source":["def indexesFromSentence(lang, sentence):\n","    return [lang.word2index[word] for word in sentence.split(' ')]\n","\n","\n","def tensorFromSentence(lang, sentence):\n","    indexes = indexesFromSentence(lang, sentence)\n","    indexes.append(EOS_token)\n","    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n","\n","\n","def tensorsFromPair(pair):\n","    input_tensor = tensorFromSentence(input_lang, pair[0])\n","    target_tensor = tensorFromSentence(output_lang, pair[1])\n","    return (input_tensor, target_tensor)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"79PfKQhqid1c","colab_type":"code","colab":{}},"source":["import time\n","import math\n","\n","\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return '%dm %ds' % (m, s)\n","\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WRCwTz4Kid1l","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","plt.switch_backend('agg')\n","import matplotlib.ticker as ticker\n","import numpy as np\n","%matplotlib inline\n","\n","\n","def showPlot(points):\n","    plt.figure()\n","    fig, ax = plt.subplots()\n","    # this locator puts ticks at regular intervals\n","    loc = ticker.MultipleLocator(base=0.2)\n","    ax.yaxis.set_major_locator(loc)\n","    plt.plot(points)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jwPXzQhVfPa1","colab_type":"text"},"source":["### **Train function (practice #2)**\n","Train the encoder and decoder network (train_op)\n","- Encode an input sentence (input_tensor)\n","- Decode target sentence from the compressed hidden vector of input sentences\n","- Compute loss and update the network parameters\n"]},{"cell_type":"code","metadata":{"id":"uYxW9Y5Sid1h","colab_type":"code","colab":{}},"source":["def train(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n","    encoder.train()\n","    decoder.train()\n","    start = time.time()\n","    plot_losses = []\n","    print_loss_total = 0  # Reset every print_every\n","    plot_loss_total = 0  # Reset every plot_every\n","\n","    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n","    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n","    \n","    # extract n_iters sentences\n","    training_pairs = [tensorsFromPair(random.choice(pairs))\n","                      for i in range(n_iters)]\n","    criterion = nn.NLLLoss()\n","\n","    for iter in range(1, n_iters + 1):\n","        training_pair = training_pairs[iter - 1]\n","        input_tensor = training_pair[0]\n","        target_tensor = training_pair[1]\n","\n","        loss = train_op(input_tensor, target_tensor, encoder,\n","                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n","        print_loss_total += loss\n","        plot_loss_total += loss\n","\n","        if iter % print_every == 0:\n","            print_loss_avg = print_loss_total / print_every\n","            print_loss_total = 0\n","            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n","                                         iter, iter / n_iters * 100, print_loss_avg))\n","\n","        if iter % plot_every == 0:\n","            plot_loss_avg = plot_loss_total / plot_every\n","            plot_losses.append(plot_loss_avg)\n","            plot_loss_total = 0\n","\n","    showPlot(plot_losses)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5qEIcuZNid1X","colab_type":"code","colab":{}},"source":["teacher_forcing_ratio = 0.5\n","\n","def train_op(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n","    input_length = input_tensor.size(0)\n","    target_length = target_tensor.size(0)\n","     \"\"\"\n","        Code Implementation Here\n","         - Encoding an input sentence (input_tensor)\n","         - Transmit all encoding output to decoder\n","    \"\"\"\n","\n","    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n","    if use_teacher_forcing:\n","        # Teacher forcing: Feed the target as the next input\n","        \"\"\"\n","        Code Implementation Here\n","         - Decoding target sentence using true target as an input\n","        \"\"\"\n","\n","    else:\n","        # Without teacher forcing: use its own predictions as the next input\n","        \"\"\"\n","        Code Implementation Here\n","         - Decoding target sentence using own prediction as an input\n","        \"\"\"\n","    \"\"\"\n","        Code Implementation Here\n","         - Compute loss and update the network parameters\n","    \"\"\"\n","    return loss.item() / target_length"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pyLVCGZOid1v","colab_type":"code","colab":{}},"source":["hidden_size = 256\n","encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n","attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n","\n","train(encoder1, attn_decoder1, 75000, print_every=5000)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vAEHok4jlAEK","colab_type":"text"},"source":["### **Test function (practice #3)**\n","When you get the a French sentence, translate it to English.\n","- Encode the French sentence and decode it using AttnDecoderRNN \n","- Output the decoded_word and attention weights that has length of decoded sentence"]},{"cell_type":"code","metadata":{"id":"LzU19L5qid1p","colab_type":"code","colab":{}},"source":["def test(encoder, decoder, sentence, max_length=MAX_LENGTH):\n","  encoder.eval()\n","  decoder.eval()\n","    with torch.no_grad():\n","        input_tensor = tensorFromSentence(input_lang, sentence)\n","        input_length = input_tensor.size()[0]\n","\n","        \"\"\"\n","        Code Implementation Here\n","          When you get the a French sentence, translate it to English.\n","          - Encode the French sentence and decode it using AttnDecoderRNN \n","          - Output the decoded_word and attention weights that has length of decoded sentence\n","        \"\"\"\n","        return decoded_words, decoder_attentions[:di + 1]\n","      "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rjy5I6xEid12","colab_type":"code","colab":{}},"source":["output_words, attentions = test(\n","    encoder1, attn_decoder1, \"je suis trop froid .\")\n","plt.matshow(attentions.numpy())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uY9FwjaVid15","colab_type":"code","colab":{}},"source":["def showAttention(input_sentence, output_words, attentions):\n","    # Set up figure with colorbar\n","    fig = plt.figure()\n","    ax = fig.add_subplot(111)\n","    cax = ax.matshow(attentions.numpy(), cmap='bone')\n","    fig.colorbar(cax)\n","\n","    # Set up axes\n","    ax.set_xticklabels([''] + input_sentence.split(' ') +\n","                       ['<EOS>'], rotation=90)\n","    ax.set_yticklabels([''] + output_words)\n","\n","    # Show label at every tick\n","    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n","    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n","\n","    plt.show()\n","\n","\n","def testAndShowAttention(input_sentence):\n","    output_words, attentions = test(\n","        encoder1, attn_decoder1, input_sentence)\n","    print('input =', input_sentence)\n","    print('output =', ' '.join(output_words))\n","    showAttention(input_sentence, output_words, attentions)\n","\n","\n","testAndShowAttention(\"elle a cinq ans de moins que moi .\")\n","\n","testAndShowAttention(\"elle est trop petit .\")\n","\n","testAndShowAttention(\"je ne crains pas de mourir .\")\n","\n","testAndShowAttention(\"c est un jeune directeur plein de talent .\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pftCiqmd954W","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}