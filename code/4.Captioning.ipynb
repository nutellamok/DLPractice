{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"4.Captioning.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"bOWXlQoIvpWD","colab_type":"code","outputId":"a53acd43-9aeb-4dbf-ef29-733531226057","executionInfo":{"status":"ok","timestamp":1582373044426,"user_tz":-540,"elapsed":16677,"user":{"displayName":"Jenny Mok","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBx6WjQhn61jI2o2RUoma2WrRvQIP2K7I8ROHvF7A=s64","userId":"14608516416463645410"}},"colab":{"base_uri":"https://localhost:8080/","height":224}},"source":["!pip install scipy==1.1.0"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting scipy==1.1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/0b/f163da98d3a01b3e0ef1cab8dd2123c34aee2bafbb1c5bffa354cc8a1730/scipy-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (31.2MB)\n","\u001b[K     |████████████████████████████████| 31.2MB 138kB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy==1.1.0) (1.17.5)\n","\u001b[31mERROR: plotnine 0.6.0 has requirement scipy>=1.2.0, but you'll have scipy 1.1.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Installing collected packages: scipy\n","  Found existing installation: scipy 1.4.1\n","    Uninstalling scipy-1.4.1:\n","      Successfully uninstalled scipy-1.4.1\n","Successfully installed scipy-1.1.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-D3NfS70vrYf","colab_type":"code","colab":{}},"source":["!git clone https://github.com/nutellamok/helper.git\n","!wget http://data.snu.ac.kr/caption_dataset.tar.gz\n","!wget http://data.snu.ac.kr/test_image.tar.gz"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qwS6GzARO67o","colab_type":"code","colab":{}},"source":["!tar -zxvf caption_dataset.tar.gz\n","!tar -zxvf test_image.tar.gz"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bn7nflRBuX7b","colab_type":"code","colab":{}},"source":["import time\n","import torch.backends.cudnn as cudnn\n","import torch.optim\n","import torch.utils.data\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch import nn\n","import torch.nn.functional as F\n","from torch.nn.utils.rnn import pack_padded_sequence\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import matplotlib.cm as cm\n","import skimage.transform\n","from scipy.misc import imread, imresize\n","from helper.datasets import *\n","from helper.utils import *\n","from nltk.translate.bleu_score import corpus_bleu"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"P2bjVLdjuX7f","colab_type":"code","colab":{}},"source":["class Encoder(nn.Module):\n","    \"\"\"\n","    Encoder.\n","    \"\"\"\n","\n","    def __init__(self, encoded_image_size=14):\n","        super(Encoder, self).__init__()\n","        \"\"\"\n","        TODO\n","        \"\"\"\n","\n","    def forward(self, images):\n","        \"\"\"\n","        TODO\n","        \"\"\"\n","        return out\n","\n","    def fine_tune(self, fine_tune=True):\n","        \"\"\"\n","        TODO\n","        \"\"\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vXz4OpETuX7h","colab_type":"code","colab":{}},"source":["class Attention(nn.Module):\n","    \"\"\"\n","    Attention Network.\n","    \"\"\"\n","\n","    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n","        \"\"\"\n","        :param encoder_dim: feature size of encoded images\n","        :param decoder_dim: size of decoder's RNN\n","        :param attention_dim: size of the attention network\n","        TODO\n","        \"\"\"\n","\n","    def forward(self, encoder_out, decoder_hidden):\n","        \"\"\"\n","        Forward propagation.\n","\n","        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n","        :param decoder_hidden: previous decoder output, a tensor of dimension (batch_size, decoder_dim)\n","        :return: attention weighted encoding, weights\n","        TODO\n","        \"\"\"\n","\n","        return attention_weighted_encoding, alpha"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4f6-lubIuX7k","colab_type":"code","colab":{}},"source":["class DecoderWithAttention(nn.Module):\n","    \"\"\"\n","    Decoder.\n","    \"\"\"\n","\n","    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=2048, dropout=0.5):\n","        \"\"\"\n","        :param attention_dim: size of attention network\n","        :param embed_dim: embedding size\n","        :param decoder_dim: size of decoder's RNN\n","        :param vocab_size: size of vocabulary\n","        :param encoder_dim: feature size of encoded images\n","        :param dropout: dropout\n","        TODO\n","        \"\"\"\n","\n","    def init_weights(self):\n","        \"\"\"\n","        Initializes some parameters with values from the uniform distribution, for easier convergence.\n","        TODO\n","        \"\"\"\n","\n","    def load_pretrained_embeddings(self, embeddings):\n","        \"\"\"\n","        Loads embedding layer with pre-trained embeddings.\n","\n","        :param embeddings: pre-trained embeddings\n","        TODO\n","        \"\"\"\n","\n","    def fine_tune_embeddings(self, fine_tune=True):\n","        \"\"\"\n","        Allow fine-tuning of embedding layer? (Only makes sense to not-allow if using pre-trained embeddings).\n","\n","        :param fine_tune: Allow?\n","        TODO\n","        \"\"\"\n","\n","    def init_hidden_state(self, encoder_out):\n","        \"\"\"\n","        Creates the initial hidden and cell states for the decoder's LSTM based on the encoded images.\n","\n","        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n","        :return: hidden state, cell state\n","        TODO\n","        \"\"\"\n","        return h, c\n","\n","    def forward(self, encoder_out, encoded_captions, caption_lengths):\n","        \"\"\"\n","        Forward propagation.\n","\n","        :param encoder_out: encoded images, a tensor of dimension (batch_size, enc_image_size, enc_image_size, encoder_dim)\n","        :param encoded_captions: encoded captions, a tensor of dimension (batch_size, max_caption_length)\n","        :param caption_lengths: caption lengths, a tensor of dimension (batch_size, 1)\n","        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices\n","        TODO\n","        \"\"\"\n","        return predictions, encoded_captions, decode_lengths, alphas, sort_ind"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"E1PugXXQuX7m","colab_type":"code","colab":{}},"source":["# Data parameters\n","data_folder = './caption_datasets/'  # folder with data files saved by create_input_files.py\n","data_name = 'flickr8k_processed'  # base name shared by data files\n","\n","# Model parameters\n","emb_dim = 512  # dimension of word embeddings\n","attention_dim = 512  # dimension of attention linear layers\n","decoder_dim = 512  # dimension of decoder RNN\n","dropout = 0.5\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # sets device for model and PyTorch tensors\n","\n","# Training parameters\n","start_epoch = 0\n","epochs = 2  # number of epochs to train for (if early stopping is not triggered)\n","epochs_since_improvement = 0  # keeps track of number of epochs since there's been an improvement in validation BLEU\n","batch_size = 32\n","workers = 1  # for data-loading; right now, only 1 works with h5py\n","encoder_lr = 1e-4  # learning rate for encoder if fine-tuning\n","decoder_lr = 4e-4  # learning rate for decoder\n","grad_clip = 5.  # clip gradients at an absolute value of\n","alpha_c = 1.  # regularization parameter for 'doubly stochastic attention', as in the paper\n","best_bleu4 = 0.  # BLEU-4 score right now\n","print_freq = 100  # print training/validation stats every __ batches\n","fine_tune_encoder = False  # fine-tune encoder?\n","checkpoint = None  # path to checkpoint, None if none"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"svBdw9AxuX7p","colab_type":"code","colab":{}},"source":["def train(train_loader, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, epoch):\n","    \"\"\"\n","    Performs one epoch's training.\n","\n","    :param train_loader: DataLoader for training data\n","    :param encoder: encoder model\n","    :param decoder: decoder model\n","    :param criterion: loss layer\n","    :param encoder_optimizer: optimizer to update encoder's weights (if fine-tuning)\n","    :param decoder_optimizer: optimizer to update decoder's weights\n","    :param epoch: epoch number\n","    TODO\n","    \"\"\"\n","        # Print status\n","        if i % print_freq == 0:\n","            print('Epoch: [{0}][{1}/{2}]\\t'\n","                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n","                  'Data Load Time {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n","                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n","                  'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})'.format(epoch, i, len(train_loader),\n","                                                                          batch_time=batch_time,\n","                                                                          data_time=data_time, loss=losses,\n","                                                                          top5=top5accs))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nzMnHSUjuX7s","colab_type":"code","colab":{}},"source":["def validate(val_loader, encoder, decoder, criterion):\n","    \"\"\"\n","    Performs one epoch's validation.\n","\n","    :param val_loader: DataLoader for validation data.\n","    :param encoder: encoder model\n","    :param decoder: decoder model\n","    :param criterion: loss layer\n","    :return: BLEU-4 score\n","    \"\"\"\n","    decoder.eval()  # eval mode (no dropout or batchnorm)\n","    if encoder is not None:\n","        encoder.eval()\n","\n","    batch_time = AverageMeter()\n","    losses = AverageMeter()\n","    top5accs = AverageMeter()\n","\n","    start = time.time()\n","\n","    references = list()  # references (true captions) for calculating BLEU-4 score\n","    hypotheses = list()  # hypotheses (predictions)\n","\n","    # explicitly disable gradient calculation to avoid CUDA memory error\n","    # solves the issue #57\n","    with torch.no_grad():\n","        # Batches\n","        for i, (imgs, caps, caplens, allcaps) in enumerate(val_loader):\n","\n","            # Move to device, if available\n","            imgs = imgs.to(device)\n","            caps = caps.to(device)\n","            caplens = caplens.to(device)\n","\n","            # Forward prop.\n","            if encoder is not None:\n","                imgs = encoder(imgs)\n","            scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(imgs, caps, caplens)\n","\n","            # Since we decoded starting with <start>, the targets are all words after <start>, up to <end>\n","            targets = caps_sorted[:, 1:]\n","\n","            # Remove timesteps that we didn't decode at, or are pads\n","            # pack_padded_sequence is an easy trick to do this\n","            scores_copy = scores.clone()\n","            scores = pack_padded_sequence(scores, decode_lengths, batch_first=True)\n","            targets = pack_padded_sequence(targets, decode_lengths, batch_first=True)\n","\n","            # Calculate loss\n","            loss = criterion(scores[0], targets[0])\n","\n","            # Add doubly stochastic attention regularization\n","            loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n","\n","            # Keep track of metrics\n","            losses.update(loss.item(), sum(decode_lengths))\n","            top5 = accuracy(scores[0], targets[0], 5)\n","            top5accs.update(top5, sum(decode_lengths))\n","            batch_time.update(time.time() - start)\n","\n","            start = time.time()\n","\n","            if i % print_freq == 0:\n","                print('Validation: [{0}/{1}]\\t'\n","                      'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n","                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n","                      'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})\\t'.format(i, len(val_loader), batch_time=batch_time,\n","                                                                                loss=losses, top5=top5accs))\n","\n","            # Store references (true captions), and hypothesis (prediction) for each image\n","            # If for n images, we have n hypotheses, and references a, b, c... for each image, we need -\n","            # references = [[ref1a, ref1b, ref1c], [ref2a, ref2b], ...], hypotheses = [hyp1, hyp2, ...]\n","\n","            # References\n","            allcaps = allcaps[sort_ind]  # because images were sorted in the decoder\n","            for j in range(allcaps.shape[0]):\n","                img_caps = allcaps[j].tolist()\n","                img_captions = list(\n","                    map(lambda c: [w for w in c if w not in {word_map['<start>'], word_map['<pad>']}],\n","                        img_caps))  # remove <start> and pads\n","                references.append(img_captions)\n","\n","            # Hypotheses\n","            _, preds = torch.max(scores_copy, dim=2)\n","            preds = preds.tolist()\n","            temp_preds = list()\n","            for j, p in enumerate(preds):\n","                temp_preds.append(preds[j][:decode_lengths[j]])  # remove pads\n","            preds = temp_preds\n","            hypotheses.extend(preds)\n","\n","            assert len(references) == len(hypotheses)\n","\n","        # Calculate BLEU-4 scores\n","        bleu4 = corpus_bleu(references, hypotheses)\n","\n","        print(\n","            '\\n * LOSS - {loss.avg:.3f}, TOP-5 ACCURACY - {top5.avg:.3f}, BLEU-4 - {bleu}\\n'.format(\n","                loss=losses,\n","                top5=top5accs,\n","                bleu=bleu4))\n","\n","    return bleu4"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oOvrSgTXuX7u","colab_type":"code","colab":{}},"source":["global best_bleu4, epochs_since_improvement, checkpoint, start_epoch, fine_tune_encoder, data_name, word_map\n","\n","# Read word map\n","word_map_file = os.path.join(data_folder, 'WORDMAP_' + data_name + '.json')\n","with open(word_map_file, 'r') as j:\n","    word_map = json.load(j)\n","\n","# Initialize / load checkpoint\n","if checkpoint is None:\n","    decoder = DecoderWithAttention(attention_dim=attention_dim,\n","                                   embed_dim=emb_dim,\n","                                   decoder_dim=decoder_dim,\n","                                   vocab_size=len(word_map),\n","                                   dropout=dropout)\n","    decoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, decoder.parameters()),\n","                                         lr=decoder_lr)\n","    encoder = Encoder()\n","    encoder.fine_tune(fine_tune_encoder)\n","    encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n","                                         lr=encoder_lr) if fine_tune_encoder else None\n","\n","else:\n","    checkpoint = torch.load(checkpoint)\n","    start_epoch = checkpoint['epoch'] + 1\n","    epochs_since_improvement = checkpoint['epochs_since_improvement']\n","    best_bleu4 = checkpoint['bleu-4']\n","    decoder = checkpoint['decoder']\n","    decoder_optimizer = checkpoint['decoder_optimizer']\n","    encoder = checkpoint['encoder']\n","    encoder_optimizer = checkpoint['encoder_optimizer']\n","    if fine_tune_encoder is True and encoder_optimizer is None:\n","        encoder.fine_tune(fine_tune_encoder)\n","        encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n","                                             lr=encoder_lr)\n","\n","# Move to GPU, if available\n","decoder = decoder.to(device)\n","encoder = encoder.to(device)\n","\n","# Loss function\n","criterion = nn.CrossEntropyLoss().to(device)\n","\n","# Custom dataloaders\n","normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                 std=[0.229, 0.224, 0.225])\n","train_loader = torch.utils.data.DataLoader(\n","    CaptionDataset(data_folder, data_name, 'VAL', transform=transforms.Compose([normalize])),\n","    batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True)\n","val_loader = torch.utils.data.DataLoader(\n","    CaptionDataset(data_folder, data_name, 'TEST', transform=transforms.Compose([normalize])),\n","    batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True)\n","\n","# Epochs\n","for epoch in range(start_epoch, epochs):\n","\n","    # Decay learning rate if there is no improvement for 8 consecutive epochs, and terminate training after 20\n","    if epochs_since_improvement == 20:\n","        break\n","    if epochs_since_improvement > 0 and epochs_since_improvement % 8 == 0:\n","        adjust_learning_rate(decoder_optimizer, 0.8)\n","        if fine_tune_encoder:\n","            adjust_learning_rate(encoder_optimizer, 0.8)\n","\n","    # One epoch's training\n","    train(train_loader=train_loader,\n","          encoder=encoder,\n","          decoder=decoder,\n","          criterion=criterion,\n","          encoder_optimizer=encoder_optimizer,\n","          decoder_optimizer=decoder_optimizer,\n","          epoch=epoch)\n","\n","    # One epoch's validation\n","    recent_bleu4 = validate(val_loader=val_loader,\n","                            encoder=encoder,\n","                            decoder=decoder,\n","                            criterion=criterion)\n","\n","    # Check if there was an improvement\n","    is_best = recent_bleu4 > best_bleu4\n","    best_bleu4 = max(recent_bleu4, best_bleu4)\n","    if not is_best:\n","        epochs_since_improvement += 1\n","        print(\"\\nEpochs since last improvement: %d\\n\" % (epochs_since_improvement,))\n","    else:\n","        epochs_since_improvement = 0\n","\n","    # Save checkpoint\n","    save_checkpoint(data_name, epoch, epochs_since_improvement, encoder, decoder, encoder_optimizer,\n","                    decoder_optimizer, recent_bleu4, is_best)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WL5y_YrwuX7x","colab_type":"code","colab":{}},"source":["def caption_image_beam_search(encoder, decoder, image_path, word_map, beam_size=3):\n","    \"\"\"\n","    Reads an image and captions it with beam search.\n","\n","    :param encoder: encoder model\n","    :param decoder: decoder model\n","    :param image_path: path to image\n","    :param word_map: word map\n","    :param beam_size: number of sequences to consider at each decode-step\n","    :return: caption, weights for visualization\n","    \"\"\"\n","\n","    k = beam_size\n","    vocab_size = len(word_map)\n","\n","    # Read image and process\n","    img = imread(image_path)\n","    if len(img.shape) == 2:\n","        img = img[:, :, np.newaxis]\n","        img = np.concatenate([img, img, img], axis=2)\n","    img = imresize(img, (256, 256))\n","    img = img.transpose(2, 0, 1)\n","    img = img / 255.\n","    img = torch.FloatTensor(img).to(device)\n","    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                     std=[0.229, 0.224, 0.225])\n","    transform = transforms.Compose([normalize])\n","    image = transform(img)  # (3, 256, 256)\n","\n","    # Encode\n","    image = image.unsqueeze(0)  # (1, 3, 256, 256)\n","    encoder_out = encoder(image)  # (1, enc_image_size, enc_image_size, encoder_dim)\n","    enc_image_size = encoder_out.size(1)\n","    encoder_dim = encoder_out.size(3)\n","\n","    # Flatten encoding\n","    encoder_out = encoder_out.view(1, -1, encoder_dim)  # (1, num_pixels, encoder_dim)\n","    num_pixels = encoder_out.size(1)\n","\n","    # We'll treat the problem as having a batch size of k\n","    encoder_out = encoder_out.expand(k, num_pixels, encoder_dim)  # (k, num_pixels, encoder_dim)\n","\n","    # Tensor to store top k previous words at each step; now they're just <start>\n","    k_prev_words = torch.LongTensor([[word_map['<start>']]] * k).to(device)  # (k, 1)\n","\n","    # Tensor to store top k sequences; now they're just <start>\n","    seqs = k_prev_words  # (k, 1)\n","\n","    # Tensor to store top k sequences' scores; now they're just 0\n","    top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n","\n","    # Tensor to store top k sequences' alphas; now they're just 1s\n","    seqs_alpha = torch.ones(k, 1, enc_image_size, enc_image_size).to(device)  # (k, 1, enc_image_size, enc_image_size)\n","\n","    # Lists to store completed sequences, their alphas and scores\n","    complete_seqs = list()\n","    complete_seqs_alpha = list()\n","    complete_seqs_scores = list()\n","\n","    # Start decoding\n","    step = 1\n","    h, c = decoder.init_hidden_state(encoder_out)\n","\n","    # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n","    while True:\n","\n","        embeddings = decoder.embedding(k_prev_words).squeeze(1)  # (s, embed_dim)\n","\n","        awe, alpha = decoder.attention(encoder_out, h)  # (s, encoder_dim), (s, num_pixels)\n","\n","        alpha = alpha.view(-1, enc_image_size, enc_image_size)  # (s, enc_image_size, enc_image_size)\n","\n","        gate = decoder.sigmoid(decoder.f_beta(h))  # gating scalar, (s, encoder_dim)\n","        awe = gate * awe\n","\n","        h, c = decoder.decode_step(torch.cat([embeddings, awe], dim=1), (h, c))  # (s, decoder_dim)\n","\n","        scores = decoder.fc(h)  # (s, vocab_size)\n","        scores = F.log_softmax(scores, dim=1)\n","\n","        # Add\n","        scores = top_k_scores.expand_as(scores) + scores  # (s, vocab_size)\n","\n","        # For the first step, all k points will have the same scores (since same k previous words, h, c)\n","        if step == 1:\n","            top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)  # (s)\n","        else:\n","            # Unroll and find top scores, and their unrolled indices\n","            top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)  # (s)\n","\n","        # Convert unrolled indices to actual indices of scores\n","        prev_word_inds = top_k_words / vocab_size  # (s)\n","        next_word_inds = top_k_words % vocab_size  # (s)\n","\n","        # Add new words to sequences, alphas\n","        seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n","        seqs_alpha = torch.cat([seqs_alpha[prev_word_inds], alpha[prev_word_inds].unsqueeze(1)],\n","                               dim=1)  # (s, step+1, enc_image_size, enc_image_size)\n","\n","        # Which sequences are incomplete (didn't reach <end>)?\n","        incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n","                           next_word != word_map['<end>']]\n","        complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n","\n","        # Set aside complete sequences\n","        if len(complete_inds) > 0:\n","            complete_seqs.extend(seqs[complete_inds].tolist())\n","            complete_seqs_alpha.extend(seqs_alpha[complete_inds].tolist())\n","            complete_seqs_scores.extend(top_k_scores[complete_inds])\n","        k -= len(complete_inds)  # reduce beam length accordingly\n","\n","        # Proceed with incomplete sequences\n","        if k == 0:\n","            break\n","        seqs = seqs[incomplete_inds]\n","        seqs_alpha = seqs_alpha[incomplete_inds]\n","        h = h[prev_word_inds[incomplete_inds]]\n","        c = c[prev_word_inds[incomplete_inds]]\n","        encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n","        top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n","        k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n","\n","        # Break if things have been going on too long\n","        if step > 50:\n","            break\n","        step += 1\n","\n","    i = complete_seqs_scores.index(max(complete_seqs_scores))\n","    seq = complete_seqs[i]\n","    alphas = complete_seqs_alpha[i]\n","\n","    return seq, alphas"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AOud-YpJuX7z","colab_type":"code","colab":{}},"source":["def visualize_att(image_path, seq, alphas, rev_word_map, smooth=True):\n","    \"\"\"\n","    Visualizes caption with weights at every word.\n","\n","    Adapted from paper authors' repo: https://github.com/kelvinxu/arctic-captions/blob/master/alpha_visualization.ipynb\n","\n","    :param image_path: path to image that has been captioned\n","    :param seq: caption\n","    :param alphas: weights\n","    :param rev_word_map: reverse word mapping, i.e. ix2word\n","    :param smooth: smooth weights?\n","    \"\"\"\n","    image = Image.open(image_path)\n","    image = image.resize([14 * 24, 14 * 24], Image.LANCZOS)\n","\n","    words = [rev_word_map[ind] for ind in seq]\n","\n","    for t in range(len(words)):\n","        if t > 50:\n","            break\n","        plt.subplot(np.ceil(len(words) / 5.), 5, t + 1)\n","\n","        plt.text(0, 1, '%s' % (words[t]), color='black', backgroundcolor='white', fontsize=12)\n","        plt.imshow(image)\n","        current_alpha = alphas[t, :]\n","        if smooth:\n","            alpha = skimage.transform.pyramid_expand(current_alpha.numpy(), upscale=24, sigma=8)\n","        else:\n","            alpha = skimage.transform.resize(current_alpha.numpy(), [14 * 24, 14 * 24])\n","        if t == 0:\n","            plt.imshow(alpha, alpha=0)\n","        else:\n","            plt.imshow(alpha, alpha=0.8)\n","        plt.set_cmap(cm.Greys_r)\n","        plt.axis('off')\n","    plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UcEaTZvtuX72","colab_type":"code","colab":{}},"source":["img_dir = './Flickr8k_Dataset/test1.jpg'\n","model_dir = './BEST_checkpoint_flickr8k_processed.pth.tar'\n","wordmap_dir = './caption_datasets/WORDMAP_flickr8k_processed.json'\n","beam_size = 5\n","smooth = True\n","\n","# Load model\n","checkpoint = torch.load(model_dir)\n","decoder = checkpoint['decoder']\n","decoder = decoder.to(device)\n","decoder.eval()\n","encoder = checkpoint['encoder']\n","encoder = encoder.to(device)\n","encoder.eval()\n","\n","# Load word map (word2ix)\n","with open(wordmap_dir, 'r') as j:\n","    word_map = json.load(j)\n","rev_word_map = {v: k for k, v in word_map.items()}  # ix2word\n","\n","# Encode, decode with attention and beam search\n","seq, alphas = caption_image_beam_search(encoder, decoder, img_dir, word_map, beam_size)\n","alphas = torch.FloatTensor(alphas)\n","\n","# Visualize caption and attention of best sequence\n","visualize_att(img_dir, seq, alphas, rev_word_map, smooth)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4wPDpG3MuX74","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}