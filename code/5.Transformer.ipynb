{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"5.Transformer.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"rncxhC9lj3wJ","colab_type":"text"},"source":["## **4. Transformer**"]},{"cell_type":"code","metadata":{"id":"iuMHwAiaNBI1","colab_type":"code","colab":{}},"source":["!pip install torchtext==0.4.0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZPl_kkPUfOwc","colab_type":"code","colab":{}},"source":["import math\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import copy"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NpiuITBQffBi","colab_type":"code","colab":{}},"source":["import torchtext\n","from torchtext.data.utils import get_tokenizer\n","TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"basic_english\"),\n","                            init_token='<sos>',\n","                            eos_token='<eos>',\n","                            lower=True)\n","train_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(TEXT)\n","TEXT.build_vocab(train_txt)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","def batchify(data, bsz):\n","    data = TEXT.numericalize([data.examples[0].text])\n","    # Divide the dataset into bsz parts.\n","    nbatch = data.size(0) // bsz\n","    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n","    data = data.narrow(0, 0, nbatch * bsz)\n","    # Evenly divide the data across the bsz batches.\n","    data = data.view(bsz, -1).t().contiguous()\n","    return data.to(device)\n","\n","\n","\n","bptt = 35\n","def get_batch(source, i):\n","    seq_len = min(bptt, len(source) - 1 - i)\n","    data = source[i:i+seq_len].t()\n","    target = source[i+1:i+1+seq_len].t()\n","    return data, target.contiguous().view(-1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-SNB_HMGyRCi","colab_type":"code","colab":{}},"source":["print('Text examples : ', train_txt.examples[0].text[51:100])\n","batch_size=20\n","train_data = batchify(train_txt, batch_size)\n","val_data = batchify(val_txt, batch_size)\n","test_data = batchify(test_txt, batch_size)\n","\n","print('\\nPreprocessed train data')\n","print('Data size : ' ,train_data.size())\n","print('Examples of numericalized train data: \\n', train_data)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4KrthP_gm-i0","colab_type":"text"},"source":["### **Generate a source mask (practice #0)**\n"]},{"cell_type":"code","metadata":{"id":"jDjRBox7fRt8","colab_type":"code","colab":{}},"source":["class TransformerModel(nn.Module):\n","\n","    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n","        super(TransformerModel, self).__init__()\n","        self.model_type = 'Transformer'\n","        self.src_mask = None\n","        self.pos_encoder = PositionalEncoding(ninp, dropout)\n","        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n","        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n","        self.encoder = nn.Embedding(ntoken, ninp)\n","        self.ninp = ninp\n","        self.decoder = nn.Linear(ninp, ntoken)\n","\n","        self.init_weights()\n","\n","    def _generate_square_subsequent_mask(self, sz):\n","        \"\"\"\n","        Code Implementation Here\n","         - Make a mask that has zero value at upper triangle \n","        \"\"\"\n","        return mask\n","\n","    def init_weights(self):\n","        initrange = 0.1\n","        self.encoder.weight.data.uniform_(-initrange, initrange)\n","        self.decoder.bias.data.zero_()\n","        self.decoder.weight.data.uniform_(-initrange, initrange)\n","\n","    def forward(self, src):\n","        if self.src_mask is None or self.src_mask.size(0) != len(src):\n","            device = src.device\n","            mask = self._generate_square_subsequent_mask(src.size(1)).to(device)\n","            self.src_mask = mask\n","\n","        src = self.encoder(src) * math.sqrt(self.ninp)\n","        src = self.pos_encoder(src)\n","        output = self.transformer_encoder(src, self.src_mask)\n","        output = self.decoder(output)\n","        return output"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_Xu-gERzmH3I","colab_type":"text"},"source":["### **Positional Encoding (practice #1)**\n","\n"]},{"cell_type":"code","metadata":{"id":"szLyp2OtfT9a","colab_type":"code","colab":{}},"source":["def _get_clones(module, N):\n","    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n","\n","def _get_activation_fn(activation):\n","    if activation == \"relu\":\n","        return F.relu\n","    elif activation == \"gelu\":\n","        return F.gelu\n","    else:\n","        raise RuntimeError(\"activation should be relu/gelu, not %s.\" % activation)\n","\n","class PositionalEncoding(nn.Module):\n","\n","    def __init__(self, d_model, dropout=0.1, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        \"\"\"\n","        Code Implementation Here\n","         - Define a poisition encoding matrix\n","        \"\"\"\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Code Implementation Here\n","         - Return x which is applied position information\n","        \"\"\"\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BhnBDbwbmPl1","colab_type":"text"},"source":["### **Transformer Encoder Layer (practice #2)**\n"]},{"cell_type":"code","metadata":{"id":"ks-WI6difY7z","colab_type":"code","colab":{}},"source":["class TransformerEncoder(nn.Module):\n","    def __init__(self, encoder_layer, num_layers, norm=None):\n","        super(TransformerEncoder, self).__init__()\n","        self.layers = _get_clones(encoder_layer, num_layers)\n","        self.num_layers = num_layers\n","        self.norm = norm\n","\n","    def forward(self, src, mask=None):\n","        output = src\n","\n","        for i in range(self.num_layers):\n","            output = self.layers[i](output, src_mask=mask)\n","\n","        if self.norm:\n","            output = self.norm(output)\n","\n","        return output\n","\n","class TransformerEncoderLayer(nn.Module):\n","    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n","        super(TransformerEncoderLayer, self).__init__()\n","        \"\"\"\n","        Code Implementation Here\n","         - Define Transformer encoder layer\n","        \"\"\"\n","       \n","\n","    def forward(self, src, src_mask=None):\n","        \"\"\"\n","        Code Implementation Here\n","         - Forward pass\n","        \"\"\"\n","     "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nEh-u3bQmWpM","colab_type":"text"},"source":["### **Multi-head Attention(practice #3)**\n","\n"]},{"cell_type":"code","metadata":{"id":"UFwcZnmwfbzZ","colab_type":"code","colab":{}},"source":["class MultiheadAttention(nn.Module):\n","    def __init__(self, embed_dim, num_heads, dropout=0):\n","        super(MultiheadAttention, self).__init__()\n","        \"\"\"\n","        Code Implementation Here\n","         - Define Layers which is needed to apply MultiheadAttention\n","        \"\"\"\n","\n","    def forward(self, query, key, value, mask=None):\n","        \"\"\"\n","        Code Implementation Here\n","         - Forward pass\n","        \"\"\"\n","       \n","\n","\n","def attention(query, key, value, mask=None, dropout=None):\n","    \"\"\"\n","    Code Implementation Here\n","      - Self-attention\n","    \"\"\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wUYvLMS0twHX","colab_type":"text"},"source":["### **Train & Test function**\n","\n"]},{"cell_type":"code","metadata":{"id":"S6QYa3Bufitp","colab_type":"code","colab":{}},"source":["import time\n","def train(model, train_data, val_data, epochs):\n","    model.train() # Turn on the train mode\n","    best_val_loss = float(\"inf\")\n","    best_model = None\n","\n","    for epoch in range(1, epochs + 1):\n","        epoch_start_time = time.time()\n","        total_loss = 0.\n","        start_time = time.time()\n","        ntokens = len(TEXT.vocab.stoi)\n","        for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n","            data, targets = get_batch(train_data, i)\n","            optimizer.zero_grad()\n","            output = model(data)\n","            loss = criterion(output.view(-1, ntokens), targets)\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n","            optimizer.step()\n","\n","            total_loss += loss.item()\n","            log_interval = 200\n","            if batch % log_interval == 0 and batch > 0:\n","                cur_loss = total_loss / log_interval\n","                elapsed = time.time() - start_time\n","                print('| epoch {:3d} | {:5d}/{:5d} batches | '\n","                      'lr {:02.2f} | ms/batch {:5.2f} | '\n","                      'loss {:5.2f} | ppl {:8.2f}'.format(\n","                        epoch, batch, len(train_data) // bptt, scheduler.get_lr()[0],\n","                        elapsed * 1000 / log_interval,\n","                        cur_loss, math.exp(cur_loss)))\n","                total_loss = 0\n","                start_time = time.time()\n","\n","        val_loss = test(model, val_data)\n","        print('-' * 89)\n","        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n","              'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n","                                        val_loss, math.exp(val_loss)))\n","        print('-' * 89)\n","\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            best_model = model\n","\n","        scheduler.step()\n","\n","    return best_model\n","   \n","\n","def test(model, data_source):\n","    model.eval() # Turn on the evaluation mode\n","    total_loss = 0.\n","    ntokens = len(TEXT.vocab.stoi)\n","    with torch.no_grad():\n","        for i in range(0, data_source.size(0) - 1, bptt):\n","            data, targets = get_batch(data_source, i)\n","            output = model(data)\n","            output_flat = output.view(-1, ntokens)\n","            total_loss += data.size(1) * criterion(output_flat, targets).item()\n","    return total_loss / (len(data_source) - 1)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qzcBAGsdtqfH","colab_type":"code","colab":{}},"source":["epochs = 3 # The number of epochs\n","ntokens = len(TEXT.vocab.stoi) # the size of vocabulary\n","emsize = 200 # embedding dimension\n","nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n","nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n","nhead = 2 # the number of heads in the multiheadattention models\n","dropout = 0.2 # the dropout value\n","batch_size = 20\n","eval_batch_size = 10\n","train_data = batchify(train_txt, batch_size)\n","val_data = batchify(val_txt, eval_batch_size)\n","test_data = batchify(test_txt, eval_batch_size)\n","\n","model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","lr = 5.0 # learning rate\n","optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n","best_model = train(model, train_data, val_data, epochs)\n","\n","test_loss = test(best_model, test_data)\n","print('=' * 89)\n","print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n","    test_loss, math.exp(test_loss)))\n","print('=' * 89)"],"execution_count":0,"outputs":[]}]}